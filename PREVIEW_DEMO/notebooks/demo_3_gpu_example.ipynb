{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "90f31c73-2e0d-4889-9f11-e150b1b04f4d",
      "metadata": {
        "collapsed": false,
        "resultHeight": 690,
        "codeCollapsed": true
      },
      "source": [
        "# GPU Based XGBoost Training\n",
        "## In the following notebook we will leverage Snowpark Container Services (SPCS) to run a notebook on a GPU cluster within Snowflake\n",
        "\n",
        "### * Workflow* \n",
        "- Inspect GPU resources available - for this exercise we will use four NVIDIA A10G GPUs\n",
        "- Load in data from Snowflake table\n",
        "- Set up data for modeling\n",
        "- Train two XGBoost models - one trained with open source xgboost (single GPU) and one distributing across the full GPU cluster\n",
        "- Log the model into Snowflake's model registry then test out built-in inference and explainability capabilities on the model object\n",
        "\n",
        "### * Key Takeaways* \n",
        "- SPCS allows users to run notebook workloads that execute on containers, rather than virtual warehouses in Snowflake\n",
        "- While Open Source XGBoost is compatible with GPUs, by default it is restricted to a single GPU. Snowflake enables users to easily distribute their training jobs across all available GPUs which can greatly speed up execution time 🔥\n",
        "- Snowflake's model registry provides a secure and flexible framework for users to deploy, track, and access models\n",
        "- Bringing in third party python libraries offers flexibility to leverage great contirbutions to the OSS ecosystem\n",
        "\n",
        "\n",
        "### Note - In order to successfully run !pip installs make sure you have enabled the external access integration with pypi\n",
        "- Do so by clicking on the drop down of the 🟢 Active kernel settings button, clicking Edit Compute Settings, then turning on the PYPI_ACCESS_INTEGRATION radio button in the external access tab"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1f17f51-9e63-48f9-b220-997caa1e63c1",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "!pip install plotnine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3775908f-ca36-4846-8f38-5adca39217f2",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "resultHeight": 84
      },
      "outputs": [],
      "source": [
        "# Import python packages\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import sys\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# xgboost libraries\n",
        "import xgboost\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Snowpark libraries & session\n",
        "from snowflake.snowpark import DataFrame\n",
        "from snowflake.snowpark.functions import col\n",
        "from snowflake.snowpark.context import get_active_session\n",
        "session = get_active_session()\n",
        "session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "resultHeight": 127
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Get the list of GPUs\n",
        "if torch.cuda.is_available():\n",
        "    # Get the number of GPUs\n",
        "    num_gpus = torch.cuda.device_count()\n",
        "\n",
        "    print(f'{num_gpus} GPU Device(s) Found')\n",
        "    # Print the list of GPUs\n",
        "    for i in range(num_gpus):\n",
        "        print(\"Name:\", torch.cuda.get_device_name(i), \"  Index:\", i)\n",
        "else:\n",
        "    print(\"No GPU available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "resultHeight": 58
      },
      "outputs": [],
      "source": [
        "#Load in data from Snowflake table into a Snowpark dataframe\n",
        "table = \"XGB_GPU_DATABASE.XGB_GPU_SCHEMA.VEHICLES_TABLE\"\n",
        "df = session.table(table)\n",
        "df.count(), len(df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6991bfd-9939-4637-9891-5f6a700332dd",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "resultHeight": 251
      },
      "outputs": [],
      "source": [
        "#Note the maximum price - a $3B car must be quite a spectacle, but we don't want to use that for our model\n",
        "df.select('PRICE').describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e95fea0-54cf-4194-b56c-a608561bc6d4",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "resultHeight": 251
      },
      "outputs": [],
      "source": [
        "#Lets filter down to cars $100k or less - note that we only filter out ~1% of our data here\n",
        "df = df.filter(col('PRICE')<100000)\n",
        "df.select('PRICE').describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0911e6e-c9cf-4d73-9f00-1c04b568ef49",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "resultHeight": 814
      },
      "outputs": [],
      "source": [
        "#View data schema\n",
        "list(df.schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab207b33-dcb4-432f-b3e3-e1b284ab0dfa",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "resultHeight": 0
      },
      "outputs": [],
      "source": [
        "#Drop some columns that won't be helpful for modeling\n",
        "drop_cols = [\"ID\",\"URL\", \"REGION_URL\", \"IMAGE_URL\", \"DESCRIPTION\", \"VIN\", \"POSTING_DATE\", 'COUNTY']\n",
        "df = df.drop(drop_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7028e36-11e1-492a-8711-500a3bd7dee4",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "resultHeight": 0
      },
      "outputs": [],
      "source": [
        "#Fill NULL values with \"NA\" for string columns and 0 for numerical columns\n",
        "from snowflake.snowpark.types import StringType\n",
        "string_cols = df.select([col.name for col in df.schema if col.datatype ==StringType()]).columns\n",
        "non_string_cols = df.drop(string_cols).columns\n",
        "\n",
        "df = df.fillna(\"NA\", subset=string_cols)\n",
        "df = df.fillna(0, subset= non_string_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb410942-555c-4bf4-9145-1e20b6e53f7c",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "resultHeight": 0
      },
      "outputs": [],
      "source": [
        "#Use pandas to find the top 100 car models and top 100 regions and cast any extra values to 'INFREQUENT' to avoid excessive dimensionality\n",
        "df_pd = df.to_pandas()\n",
        "top_n_models = df_pd.MODEL.value_counts().keys()[0:100]\n",
        "top_n_regions = df_pd.REGION.value_counts().keys()[0:100]\n",
        "df_pd['MODEL'] = df_pd.MODEL.apply(lambda x: x if x in top_n_models else 'INFREQUENT')\n",
        "df_pd['REGION'] = df_pd.REGION.apply(lambda x: x if x in top_n_regions else 'INFREQUENT')\n",
        "\n",
        "df = session.create_dataframe(df_pd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ea0b207-0f32-46eb-ae7c-1e2cb4089693",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "resultHeight": 58
      },
      "outputs": [],
      "source": [
        "#Union the data to itself a few times to go from 400k rows to 1.7M rows. This lab's purpose is to test performance so we want to have a decently large dataset!\n",
        "for i in range(1,3):\n",
        "    df = df.unionAll(df)\n",
        "\n",
        "df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a86dfd16-2fab-435a-8d84-008251951c30",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "resultHeight": 167
      },
      "outputs": [],
      "source": [
        "import snowflake.ml.modeling.preprocessing as snowml\n",
        "\n",
        "OHE_COLS = string_cols\n",
        "OHE_POST_COLS = [i+\"_OHE\" for i in OHE_COLS]\n",
        "\n",
        "\n",
        "# Encode categoricals to numeric columns\n",
        "snowml_ohe = snowml.OneHotEncoder(input_cols=OHE_COLS, output_cols = OHE_COLS, drop_input_cols=True)\n",
        "transformed_df = snowml_ohe.fit(df).transform(df)\n",
        "transformed_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "024c25ed-6cc3-414b-8360-e0b9651ef98c",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "resultHeight": 0
      },
      "outputs": [],
      "source": [
        "#Rename columns to avoid issues with \" characters later on\n",
        "\n",
        "#Create dict replacing bad column names\n",
        "renaming_dict = {}\n",
        "for n, col in enumerate(transformed_df.columns):\n",
        "    double_quote_spot = col.find('\"')\n",
        "    if double_quote_spot==0:\n",
        "        renaming_dict[col] = col[double_quote_spot+1:col.find(\"_\")]+f\"__{n}\"\n",
        "    else:\n",
        "        renaming_dict[col] = col\n",
        "\n",
        "\n",
        "#Create new df with renamed and sorted columns\n",
        "df_renamed = transformed_df.rename(renaming_dict)\n",
        "df_renamed = df_renamed.select(sorted(df_renamed.columns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e49a0391-a323-4920-b0a4-48976091f299",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "resultHeight": 0
      },
      "outputs": [],
      "source": [
        "# Split the data into train and test sets\n",
        "train, test = df_renamed.random_split(weights=[0.95, 0.05], seed=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64d96046-2502-4bfe-b8cd-6c4794d80681",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "resultHeight": 0
      },
      "outputs": [],
      "source": [
        "#Convert snowpark tables to pandas for use later on\n",
        "train_pd = train.to_pandas()\n",
        "test_pd = test.to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40833ad7-521a-49df-afce-b7046693f685",
      "metadata": {
        "collapsed": false,
        "resultHeight": 320,
        "codeCollapsed": true
      },
      "source": [
        "## Model Training\n",
        "\n",
        "### Now that our data is all set up - we will train a Distributed GPU-powered XGBoost model\n",
        "#### The parameter that instructs our model to leverage GPUs is *tree_method*. \n",
        "--- When *tree_method* is set to *hist* the model will not attempt to use GPUs\n",
        "\n",
        "--- When *tree_method* is set to *gpu_hist* the model will leverage any available GPUs found\n",
        "\n",
        "--- Snowflake offers the ability to leverage multi-GPU training (i.e. using all 4 of our A10G GPUs we have available) for optimized performance\n",
        "\n",
        "--- Open Source XGBoost will only use a single GPU"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ace6b57-2fb9-4043-af80-148a8e6adc21",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "resultHeight": 0
      },
      "outputs": [],
      "source": [
        "#Define oss xgboost model\n",
        "oss_xgb_gpu = XGBRegressor(\n",
        "    tree_method=\"gpu_hist\",\n",
        "    n_estimators=2000,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16c16e49-805f-4250-a76a-5e1bc23dd8c4",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "resultHeight": 278
      },
      "outputs": [],
      "source": [
        "#train oss xgboost model\n",
        "oss_xgb_gpu.fit(\n",
        "    X=train_pd.drop(\"PRICE\", axis=1),\n",
        "    y=train_pd[\"PRICE\"],)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6baefa85-eb56-42ac-9d06-129f009f48cd",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "resultHeight": 38
      },
      "outputs": [],
      "source": [
        "#compute predictions and performance metrics\n",
        "from sklearn.metrics import r2_score\n",
        "xgb_yhat = oss_xgb_gpu.predict(test_pd.drop(\"PRICE\",axis=1))\n",
        "print(r2_score(test_pd.PRICE, xgb_yhat))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ef8e5b7-ee5b-45ef-a1b5-0106e3f7ee55",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "resultHeight": 0
      },
      "outputs": [],
      "source": [
        "#Clear cache to make sure we have as much free memory as possible for modeling\n",
        "\n",
        "import gc\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fff58978-bdbd-44da-a659-744b0d1209f1",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "resultHeight": 0
      },
      "outputs": [],
      "source": [
        "from snowflake.ml.modeling.distributors.xgboost.xgboost_estimator import XGBEstimator, XGBScalingConfig\n",
        "from snowflake.ml.data.data_connector import DataConnector\n",
        "dc = DataConnector.from_dataframe(train)\n",
        "\n",
        "#Specify GPU usage \n",
        "gpu_scaling_config = XGBScalingConfig(use_gpu=True)\n",
        "\n",
        "#Define distributed xgb estimator\n",
        "dist_gpu_xgb = XGBEstimator(\n",
        "    params = {\"tree_method\": \"gpu_hist\",\n",
        "              \"n_estimators\":2000,},\n",
        "    scaling_config = gpu_scaling_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f4a1a6a-5b33-4996-b895-8376b1932871",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "resultHeight": 47459
      },
      "outputs": [],
      "source": [
        "#Train distributed xgb estimator\n",
        "dist_gpu_xgb.fit(dc,\n",
        "                 input_cols = train.drop(\"PRICE\").columns,\n",
        "                 label_col = \"PRICE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ba65ac8-3dd2-4d35-9e6f-daa5f205aec6",
      "metadata": {
        "collapsed": false,
        "resultHeight": 136,
        "codeCollapsed": true
      },
      "source": [
        "## While the model is training, you can see a live look at resource utilization by hovering your mouse over the 🟢 Active button that controls the kernel settings for your notebook.\n",
        "### Notice the memory, CPU utilziation and GPU utilization while the model training executes"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6e6cb26c-5ed4-4070-a647-e24fcd562f61",
      "metadata": {
        "collapsed": false,
        "resultHeight": 248,
        "codeCollapsed": true
      },
      "source": [
        "## While results aren't entirely determinstic, you should have seen a 3-4x speedup in model training time from OSS (single GPU) to Snowflake-Distributed (four GPUs) Training. \n",
        "### Note that while the wall time difference is not as pronounced, the pure training time is the key piece to consider here. \n",
        "## You can inspect the train time of your distributed training job by reading through the cell output of the above cell labeled *train_distributed_gpu_model*. Note the run time of the first and last iteration of the training job.  \n",
        "### The pure training time of the distributed model should be ~25-30 seconds (compared to ~90s for single GPU training)\n",
        "\n",
        "#### For a more comprehensive performance comparison please see this [engineering blog](https://www.snowflake.com/en/engineering-blog/machine-learning-container-runtime/) comparing the performance of various Snowflake Container Runtime functions including Distributed Multi-GPU Model Training"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68837f86-6eb9-4729-a85b-c41b35a4c8e0",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "resultHeight": 38
      },
      "outputs": [],
      "source": [
        "#Compute predictions and performance metrics\n",
        "dist_xgb_yhat = dist_gpu_xgb.predict(test_pd.drop(\"PRICE\",axis=1))\n",
        "print(r2_score(test_pd.PRICE, dist_xgb_yhat))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5edc6b25-1b4e-4c36-bc92-09b5a631cd82",
      "metadata": {
        "collapsed": false,
        "language": "python",
        "resultHeight": 917
      },
      "outputs": [],
      "source": [
        "#In our visualization below we can see that there is a reasonably tight correlation between predicted and actual prices for cars \n",
        "test_vis = test_pd[test_pd.PRICE>0]\n",
        "sns.scatterplot(x=test_vis.PRICE, y = dist_gpu_xgb.predict(test_vis.drop(\"PRICE\",axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d44e02ad-c265-4bfd-b772-f714ec5fb71a",
      "metadata": {
        "collapsed": false,
        "language": "python",
        "resultHeight": 251
      },
      "outputs": [],
      "source": [
        "#Extract xgb booster object from Snowflake optimized XGB model\n",
        "gpu_booster = dist_gpu_xgb.get_booster()\n",
        "gpu_booster.predict(xgboost.DMatrix(test_pd.drop(\"PRICE\", axis=1)))[0:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7935adb2-a4eb-4756-a52f-c49825146767",
      "metadata": {
        "collapsed": false,
        "resultHeight": 107,
        "codeCollapsed": true
      },
      "source": [
        "## Now we will log our model to Snowflake's Model Registry\n",
        "### To learn more about the Model Registry please see our [documentation](https://docs.snowflake.com/en/developer-guide/snowflake-ml/model-registry/overview)"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44cba8be-ccf1-444a-bc50-ea59fc21c97d",
      "metadata": {
        "collapsed": false,
        "language": "python",
        "resultHeight": 0
      },
      "outputs": [],
      "source": [
        "from snowflake.ml.registry import Registry\n",
        "from snowflake.ml.model import model_signature\n",
        "\n",
        "# Define model name\n",
        "model_name = \"DISTRIBUTED_XGB_ON_GPU_QUICKSTART\"\n",
        "version_name = \"DIST_XGB\"\n",
        "\n",
        "# Create a registry and log the model\n",
        "model_registry = Registry(session=session, \n",
        "                          database_name=session.get_current_database(), \n",
        "                          schema_name=session.get_current_schema())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f254bf4a-e7d7-4cde-a74b-22c4a05ecb51",
      "metadata": {
        "collapsed": false,
        "language": "python"
      },
      "outputs": [],
      "source": [
        "#Log model to Model Registry  (or retrieve model if already registered)\n",
        "try: \n",
        "    logged_model = model_registry.log_model(\n",
        "        model_name=model_name,\n",
        "        version_name = version_name,\n",
        "        model=gpu_booster,\n",
        "        sample_input_data = test.drop(\"PRICE\"),\n",
        "        target_platforms= {\"WAREHOUSE\"}\n",
        "        # options={\"cuda_version\": torch.version.cuda}) #Can add this line in for GPU inference support (see conclusion section of notebook for more info!)\n",
        "    )\n",
        "    print(\"Logged new model...\")\n",
        "except ValueError:\n",
        "    logged_model = model_registry.get_model(model_name).version(version_name)\n",
        "    print(\"Retrieved existing model!\")\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65b6bd8b-9e5d-4d0a-a9fc-001095a9964a",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "resultHeight": 0
      },
      "outputs": [],
      "source": [
        "#Run inference against model in model registry\n",
        "registry_preds = logged_model.run(test.drop(\"PRICE\"), function_name=\"PREDICT\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "022de562-14cb-4b30-acb4-745ac0cd0c36",
      "metadata": {
        "collapsed": false,
        "language": "python",
        "resultHeight": 345
      },
      "outputs": [],
      "source": [
        "#Show a few predictions\n",
        "registry_preds.select([registry_preds.columns[-1]]).show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c37a037c-9879-4be5-8cec-b263edc4050d",
      "metadata": {
        "collapsed": false,
        "language": "python",
        "resultHeight": 1011
      },
      "outputs": [],
      "source": [
        "#Use built in model registry functionality to compute Shapley values (row-level explanations)\n",
        "exps = logged_model.run(test_pd.drop(\"PRICE\",axis=1)[-1000:], function_name=\"EXPLAIN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5903202-7127-4c0e-8d4a-c3f2bde3a293",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "#Plot feature values and shapley values for YEAR\n",
        "#Note that cars from recent years cost significantly more than older cars which makes sense!\n",
        "from plotnine import ggplot, aes, geom_point, labs, theme_minimal, scale_x_continuous\n",
        "\n",
        "exps[\"YEAR\"] = test_pd.YEAR[-1000:].values\n",
        "\n",
        "p = (ggplot(exps, aes(x='YEAR', y='YEAR_explanation')) +\n",
        "     geom_point(size=3, color=\"#1E90FF\", alpha=0.25) +\n",
        "     labs(title=\"Influence of Year on Car Sales Price\",\n",
        "          x=\"Year\",\n",
        "          y=\"Year Influence\")+\n",
        "     scale_x_continuous(limits=(1980, 2022)))\n",
        "\n",
        "# Show the plot\n",
        "p.draw()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e5a3823-c58f-4e40-8758-58691725c3a0",
      "metadata": {
        "collapsed": false,
        "resultHeight": 483,
        "codeCollapsed": true
      },
      "source": [
        "# Conclusion\n",
        "## We have now completed our workflow which involved the below stages - \n",
        "### 👩‍💻 Read in data from an s3 bucket into a Snowflake Table and into our Notebook👩‍💻\n",
        "\n",
        "### 🛠️ Filtered null values, identified & selected relevant columns, performed One-hot encoding and more to get our data model-ready 🛠️\n",
        "\n",
        "### 🔮 Trained an XGBoost model on a single GPU 🔮\n",
        "\n",
        "### 🔥 Easily accelerated the model training process using Snowflake's distributed model training framework to train our XGBoost model on multiple GPUs! 🔥\n",
        "### 🚀 Logged our model to the Snowflake Model Registry where our model will be securely stored, versioned, and maintained 🚀\n",
        "### 🔎 Finally we performed model inference and explainability on our model from the Model Registry 🔎\n",
        "#### Here in our [docs](https://docs.snowflake.com/en/developer-guide/snowflake-ml/model-registry/container) you can read up on how we could take this a step further and deploy this model to a GPU-container-based inference service!"
      ],
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Streamlit Notebook",
      "name": "streamlit"
    },
    "lastEditStatus": {
      "authorEmail": "elliott.botwick@snowflake.com",
      "authorId": "5095547476787",
      "authorName": "EBOTWICK",
      "lastEditTime": 1737591895906,
      "notebookId": "ckzouxgywtuflw6zokj3",
      "sessionId": "d91e32bd-a3b2-4805-acd2-243125d14279"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}